{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import gc\nimport hyperopt\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom hyperopt import space_eval\nimport time\nimport math\nfrom hyperopt.pyll.base import scope\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm_notebook as tqdm\nimport lightgbm as lgb\nimport pprint\npp = pprint.PrettyPrinter(indent=4)\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import roc_auc_score, f1_score\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir= \"/kaggle/input/creditcardfraud\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(data_dir + \"/\" + \"creditcard.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset is available in the cleaned format with PCA applied on some unspecified underlying original varilables hidden from public due to its sensitive nature. "},{"metadata":{"trusted":true},"cell_type":"code","source":"input_cols = [\"V\" + str(x) for x in range(1,29)] + [\"Amount\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[input_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df[\"Class\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that the dataset is heavily imbalanced as there are very samples with target class value 1 than 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will balance dataset with SMOTE, which will oversample the samples that have minority class as output value by introducing new synthetic samples that have slightly different values of input variables from each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Balance dataset with SMOTE\nsm = SMOTE(random_state=7)\nX_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\nX_train_bal = pd.DataFrame(X_train_bal, columns=input_cols)\ny_train_bal = pd.Series(y_train_bal)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next let's find out the best hyperparameters for LightGBM classifier model. We will use several tuning rounds and in each round we narrow down the search range of hyperparameter space. We are using Tree-structured Parzen Estimator (TPE) algorithm to explore hyperparameter space in each round. We are using Hyperopt library where objective function calculates the negative f1 score as value to be minimized while searching for the optimal values of hyperparameters. Finally we find out the best number of iterations with reduced learning rate for gradient boosting algorithm to be used for training on entire training dataset, before evaluating its performance against test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"number_of_evals = 100\ndef find_best_params_for_lgb(X, y):\n    evaluated_point_scores = {}\n    \n    def objective(params):\n        garbage=gc.collect()\n        if (str(params) in evaluated_point_scores):\n            return evaluated_point_scores[str(params)]\n        else:          \n            kf = KFold(n_splits=2, random_state=7)\n            scores = []\n            for train_index, test_index in kf.split(X.values):                \n                X_train, X_val = X.values[train_index], X.values[test_index]\n                y_train, y_val = y.values.ravel()[train_index], y.values.ravel()[test_index]\n            \n                train_data = lgb.Dataset(X_train, \n                                label=y_train,\n                                feature_name=list(X.columns),\n                                )\n                \n                validation_data = lgb.Dataset(X_val, \n                                label=y_val,\n                                feature_name=list(X.columns),\n                                )\n                \n                evals_result = {}\n                bst = lgb.train(params, train_data, \n                                valid_sets=[train_data, validation_data], \n                                valid_names=['train', 'val'], \n                                evals_result=evals_result, \n                                num_boost_round=10000,\n                                early_stopping_rounds=100,\n                                verbose_eval=None,\n                               )\n\n                y_val_preds = np.where(bst.predict(X_val) > 0.5, 1, 0)\n                score = f1_score(y_val, y_val_preds)\n                scores.append(score)\n                \n#             print(\"Evaluating params:\")\n#             pp.pprint(params)\n            socre=np.mean(scores).item(0)\n#             print(\"f1: \" + str(score))\n            evaluated_point_scores[str(params)] = -score\n            return -score\n        \n    # This parameter tuner is able to narrow down parameter search space after each tuning round\n    parameters_tuned = {\n                        \"num_leaves\": (32, 1024),     \n                        \"max_depth\": (6, 64),\n                        \"feature_fraction\": (0.9, 1.0),\n                        \"max_bin\": (50, 250),\n                        \"bagging_fraction\": (0.7, 1.0),\n                        \"lambda_l1\": (1.0, 10.0),\n                        \"lambda_l2\": (1.0, 100.0)\n                        }\n    best_params = None\n    number_of_tuning_rounds = 3\n    for tuning_round in range(number_of_tuning_rounds,0,-1):     \n        \n        # Narrowing down the parameter space to be explored in this round of parameter tuning      \n        \n        parameter_space_range = {     \n        } \n        for parameter in parameters_tuned.keys():\n            if best_params is not None:\n                prev_best = best_params[parameter]\n                (lower_val, upper_val) = parameters_tuned[parameter]\n                range_one_side = (tuning_round/number_of_tuning_rounds) * ((upper_val - lower_val)/2.0)\n                parameter_space_range[parameter] = (max(lower_val, prev_best - range_one_side),  min(upper_val, prev_best + range_one_side))\n            else:\n                # For the initial tuning round\n                parameter_space_range = parameters_tuned.copy()\n                \n        param_space = {\n            'objective': hp.choice(\"objective\", [\"binary\"]),        \n            \"max_depth\": scope.int(hp.quniform(\"max_depth\", parameter_space_range[\"max_depth\"][0], parameter_space_range[\"max_depth\"][1], 1)),\n            \"learning_rate\": hp.choice(\"learning_rate\", [0.2]),\n            \"num_leaves\": scope.int(hp.quniform(\"num_leaves\", parameter_space_range[\"num_leaves\"][0], parameter_space_range[\"num_leaves\"][1], 10)),   \n            \"max_bin\": scope.int(hp.quniform(\"max_bin\", parameter_space_range[\"max_bin\"][0], parameter_space_range[\"max_bin\"][1], 10)),\n            \"bagging_fraction\": hp.quniform('bagging_fraction', parameter_space_range[\"bagging_fraction\"][0], parameter_space_range[\"bagging_fraction\"][1], 0.05),\n            \"feature_fraction\": hp.uniform(\"feature_fraction\", parameter_space_range[\"feature_fraction\"][0], parameter_space_range[\"feature_fraction\"][1]),\n            \"bagging_freq\": hp.choice(\"bagging_freq\", [1]),\n            \"lambda_l1\": hp.quniform('lambda_l1', parameter_space_range[\"lambda_l1\"][0], parameter_space_range[\"lambda_l1\"][1], 1),        \n            \"lambda_l2\": hp.quniform('lambda_l2', parameter_space_range[\"lambda_l2\"][0], parameter_space_range[\"lambda_l2\"][1], 5),\n            \"loss_function\": hp.choice(\"loss_function\", [\"binary_error\"]), \n            \"eval_metric\": hp.choice(\"eval_metric\", [\"binary_error\"]),\n            \"metric\": hp.choice(\"metric\", [\"binary_error\"]),\n            \"random_state\": hp.choice(\"random_state\", [7]),\n            \"verbose\": hp.choice(\"verbose\", [None])\n        }\n        start_time = time.time()\n        best_params = space_eval(\n            param_space, \n            fmin(objective, \n                 param_space, \n                 algo=hyperopt.tpe.suggest,\n                 max_evals=number_of_evals))\n    \n    # Finding best number of iterations with learning rate 0.1\n    best_params[\"learning_rate\"] = 0.1\n\n    kf = KFold(n_splits=5)\n\n    num_iterations_array = []\n    for train_index, test_index in kf.split(X.values):                \n        X_train, X_val = X.values[train_index], X.values[test_index]\n        y_train, y_val = y.values.ravel()[train_index], y.values.ravel()[test_index]\n\n        train_data = lgb.Dataset(X_train, \n                        label=y_train,\n                        feature_name=list(X.columns),\n                        )\n\n        validation_data = lgb.Dataset(X_val, \n                        label=y_val,\n                        feature_name=list(X.columns),\n                        )\n\n        evals_result = {}\n        bst = lgb.train(best_params, train_data, \n                        valid_sets=[train_data, validation_data], \n                        valid_names=['train', 'val'], \n                        evals_result=evals_result, \n                        num_boost_round=10000,\n                        early_stopping_rounds=100,\n                        verbose_eval=None,\n                       )\n\n        num_iterations_array.append(bst.best_iteration)        \n\n    best_params[\"num_iterations\"] = int(np.mean(num_iterations_array).item(0))        \n    print (\"Best Hyperparameters found:\")\n    pp.pprint(best_params)\n    return best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = find_best_params_for_lgb(X=X_train_bal, y=y_train_bal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = lgb.Dataset(X_train_bal.values, \n                            label=y_train_bal.values.ravel(),\n                            feature_name=list(X_train_bal.columns),\n                        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bst = lgb.train(best_params, train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_probs = bst.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating AUC ROC score"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_score = roc_auc_score(y_test, y_probs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating F1-Score with sample representing a ****fraudulant transaction considered as positive sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = np.where(y_probs > 0.5, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1 = f1_score(y_test, y_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance of the model can be further improved by exploring the Hyperparameter space at more granuarlity level. This can be achieved by:\n\n1. Increasing number of hyperparameter value combinations evaluated at each tuning round (currently we are evaluating 100 combinations per tuning round)\n2. Number of tuning rounds, where each tuning round narrows down the parameter value range (currently we are using 3 tuning rounds)\n\nThis will take more execution time to explore the hyperparameter space to find the optimal parameters.\n\nBayesian Optimization technique can also be used to narrow down search space of Hyperparams."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}